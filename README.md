# ska-dlm-client

Client(s) for the SKA Data Lifecycle Manager.

The ska_dlm_client package provides two Command Line Interfaces (CLIs) for the [SKA Data Lifecycle Managment system (DLM)](https://developer.skao.int/projects/ska-data-lifecycle/en/latest/overview/index.html), enabling ingestion of data products and their associated metadata. The Kafka client is being used in operations to ingest visibility data products created by the SDP receive pipeline, the Directory Watcher client is used in a more generic way to watch a configured directory and ingest new products as soon as they appear there. These two CLIs are based on the autogenerated openAPI DLM REST client interface.

## Documentation

[![Documentation Status](https://readthedocs.org/projects/ska-telescope-ska-dlm-client/badge/?version=latest)](https://developer.skao.int/projects/ska-dlm-client/en/latest/?badge=latest)

Full documentation for this project, particularly the auto-generated OpenAPI libraries, can be found in the `docs` folder, or browsed in the SKA development portal:

* [ska-dlm-client documentation](https://developer.skatelescope.org/projects/ska-dlm-client/en/latest/index.html "SKA Developer Portal: ska-dlm-client documentation")

## Execution Modes

We are providing two SKA DLM client CLIs for different scenarios:

* Directory Watcher: where the client is triggered by the creation of a file in a specified directory
* Kafka Watcher: where the client is triggered by incoming kafka messages on a specified topic

In both cases the SKA DLM client library API is then used to ingest (register) the new data product
into the DLM system.

## Directory Watcher

The directory_watcher will watch a given directory and add the file or directory to the DLM.

As parameters the directory_watcher requires
- A directory to watch
- The storage name to use for registering the files
- The URL to the DLM server (ingest-server-url)

Optional parameters include
- Prefix to add to uri of data items being registered
- Use a polling watcher instead of the iNotify event based watcher
- Whether to use the status file
- Reload the status file
- An alternative name for the status file

Calling the Directory watcher CLI:
```sh
$ dlm_directory_watcher
usage: dlm_directory_watcher [-h] -d DIRECTORY_TO_WATCH -i INGEST_SERVER_URL
                             -n STORAGE_NAME [-p REGISTER_DIR_PREFIX]
                             [--use-polling-watcher | --no-use-polling-watcher]
                             [--use-status-file | --no-use-status-file]
                             [--reload-status-file | --no-reload-status-file]
                             [--status-file-filename STATUS_FILE_FILENAME]
dlm_directory_watcher: error: the following arguments are required: -d/--directory-to-watch, -i/--ingest-server-url, -n/--storage-name
```

The additional parameters can be given for greater configuration.

## Kafka Watcher

The Kafka watcher is subscribing to a specified Kafka topic and triggers an ingestion when a compliant message is received.

As parameters the kafka_watcher requires
- A URL pointing to the Kafka broker
- A Kafka topic to watch
- The storage name to use for registering the files
- The URL to the DLM server (ingest-server-url)

Optional parameters include
- Prefix to add to uri of data items being registered
- Use a polling watcher instead of the iNotify event based watcher
- Whether to use the status file
- Reload the status file
- An alternative name for the status file

Calling the Kafka watcher CLI:
```sh
$ dlm-kafka-watcher
usage: dlm-kafka-watcher [-h] [--kafka-topic [KAFKA_TOPIC ...]] --kafka-broker-url KAFKA_BROKER_URL [KAFKA_BROKER_URL ...] --storage-name STORAGE_NAME --ingest-server-url INGEST_SERVER_URL [--check-rclone-access]
dlm-kafka-watcher: error: the following arguments are required: --kafka-broker-url, --storage-name, --ingest-server-url
```

### Metadata Handling
# Data Product Metadata

Existing metadata is loaded and the "execution block" attribute extracted.

This is based on [ADR-55 Definition of metadata for data management at AA0.5](https://confluence.skatelescope.org/display/SWSI/ADR-55+Definition+of+metadata+for+data+management+at+AA0.5).

For any dataproduct found without an identifiable associated metadata file (a file called ska-data-product.yaml) a minimal set of metadata will be generated. The
exact rules for this are a WIP.

# Deployment

The standard deployment of the ska-dlm-client within the SKA Kubernetes environment uses a set of helm charts and an associated configuration file.
For a detailed guide please check the [sample deployment documentation](https://developer.skao.int/projects/ska-data-lifecycle/en/latest/overview/index.html#sample-deployment).

# Testing

To run automated tests

```sh
make python-test
```


# OpenAPI Generated Client

```ska_dlm_client.openapi``` is an OpenAPI generated RESTful python client for accessing DLM services.

See [OpenAPI README.md](docs/src/openapi_readme.rst) for further information.

## Version

As openapi-generator was not installed via poetry the version used is shown here:

```sh
$ openapi-generator --version
openapi-generator-cli 7.9.0
  commit : 4145000
  built  : -999999999-01-01T00:00:00+18:00
  source : https://github.com/openapitools/openapi-generator
  docs   : https://openapi-generator.tech/
```

The OpenAPI generated client can be regenerated using exported OpenAPI specs from [ska-data-lifecycle](https://gitlab.com/ska-telescope/ska-data-lifecycle):

* Start the DLM services such that they can be accessed from `http://localhost`
* run `make openapi-code-from-local-dlm`
