# ska-dlm-client

Command-line clients for the SKA Data Lifecycle Management (DLM) system.

`ska_dlm_client` package provides two command-line interface (CLI) applications that enable the ingestion of data products and their associated metadata into the [SKA Data Lifecycle Management system (DLM)](https://developer.skao.int/projects/ska-data-lifecycle/en/latest/overview/index.html).

- **Kafka Client**: Actively used in operations to ingest visibility data products generated by the SDP receive pipeline.
- **Directory Watcher Client**: A general-purpose tool that monitors a specified directory and ingests new data products as they appear.

Both applications are built on top of the autogenerated OpenAPI-based DLM REST client. Direct use of the REST interface is not recommended but is available for advanced use cases.

In addition the `ska_dlm_client` package provides additional supporting applications that enable the background functions of the ingest/migration process.

## Documentation

[![Documentation Status](https://readthedocs.org/projects/ska-telescope-ska-dlm-client/badge/?version=latest)](https://developer.skao.int/projects/ska-dlm-client/en/latest/?badge=latest)

Full documentation for this project, particularly the auto-generated OpenAPI libraries, can be found in the `docs` folder, or browsed in the SKA development portal:

* [ska-dlm-client documentation](https://developer.skatelescope.org/projects/ska-dlm-client/en/latest/index.html "SKA Developer Portal: ska-dlm-client documentation")

## Execution Modes

We are providing two SKA DLM client CLIs for different scenarios:

* Directory Watcher: where the client is triggered by the creation of a file in a specified directory
* Kafka Watcher: where the client is triggered by incoming kafka messages on a specified topic

In both cases the SKA DLM client library API is then used to ingest (register) the new data product
into the DLM system.

## Directory Watcher

The directory_watcher will watch a given directory and add the file or directory to the DLM.

As parameters the directory_watcher requires
- A directory to watch
- The storage name to use for registering the files
- The URL to the DLM server (ingest-server-url)

Optional parameters include
- Prefix to add to uri of data items being registered
- Use a polling watcher instead of the iNotify event based watcher
- Whether to use the status file
- Reload the status file
- An alternative name for the status file

Calling the Directory watcher CLI:
```sh
$ dlm_directory_watcher
usage: dlm_directory_watcher [-h] -d DIRECTORY_TO_WATCH -i INGEST_SERVER_URL
                             -n STORAGE_NAME [-p REGISTER_DIR_PREFIX]
                             [--use-polling-watcher | --no-use-polling-watcher]
                             [--use-status-file | --no-use-status-file]
                             [--reload-status-file | --no-reload-status-file]
                             [--status-file-filename STATUS_FILE_FILENAME]
dlm_directory_watcher: error: the following arguments are required: -d/--directory-to-watch, -i/--ingest-server-url, -n/--storage-name
```

The additional parameters can be given for greater configuration.

## Kafka Watcher

The Kafka watcher is subscribing to a specified Kafka topic and triggers an ingestion when a compliant message is received.

As parameters the kafka_watcher requires
- A URL pointing to the Kafka broker
- A Kafka topic to watch
- The storage name to use for registering the files
- The URL to the DLM server (ingest-server-url)

Optional parameters include
- Prefix to add to uri of data items being registered
- Use a polling watcher instead of the iNotify event based watcher
- Whether to use the status file
- Reload the status file
- An alternative name for the status file

Calling the Kafka watcher CLI:
```sh
$ dlm-kafka-watcher
usage: dlm-kafka-watcher [-h] [--kafka-topic [KAFKA_TOPIC ...]] --kafka-broker-url KAFKA_BROKER_URL [KAFKA_BROKER_URL ...] --storage-name STORAGE_NAME --ingest-server-url INGEST_SERVER_URL [--check-rclone-access]
dlm-kafka-watcher: error: the following arguments are required: --kafka-broker-url, --storage-name, --ingest-server-url
```

### Metadata Handling
# Data Product Metadata

Existing metadata is loaded and the "execution block" attribute extracted.

This is based on [ADR-55 Definition of metadata for data management at AA0.5](https://confluence.skatelescope.org/display/SWSI/ADR-55+Definition+of+metadata+for+data+management+at+AA0.5).

For any dataproduct found without an identifiable associated metadata file (a file called ska-data-product.yaml) a minimal set of metadata will be generated. The
exact rules for this are a WIP.

# Deployment

The standard deployment of the ska-dlm-client within the SKA Kubernetes environment uses a set of helm charts and an associated configuration file.
For a detailed guide please check the [sample deployment documentation](https://developer.skao.int/projects/ska-data-lifecycle/en/latest/overview/index.html#sample-deployment).

# Testing

To run automated tests

```sh
make python-test
```


# OpenAPI Generated Client

```ska_dlm_client.openapi``` is an OpenAPI generated RESTful python client for accessing DLM services.

See [OpenAPI README.md](docs/src/openapi_readme.rst) for further information.

## Version

As openapi-generator was not installed via poetry the version used is shown here:

```sh
$ openapi-generator --version
openapi-generator-cli 7.9.0
  commit : 4145000
  built  : -999999999-01-01T00:00:00+18:00
  source : https://github.com/openapitools/openapi-generator
  docs   : https://openapi-generator.tech/
```

The OpenAPI generated client can be regenerated using exported OpenAPI specs from [ska-data-lifecycle](https://gitlab.com/ska-telescope/ska-data-lifecycle):

* Start the DLM services such that they can be accessed from `http://localhost`
* run `make openapi-code-from-local-dlm`
